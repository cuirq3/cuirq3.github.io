@article{wang2025materialist,
  title={Materialist: Physically based editing using single-image inverse rendering},
  author={Wang, Lezhong and Tran, Duc Minh and Cui, Ruiqi and TG, Thomson and Dahl, Anders Bjorholm and Bigdeli, Siavash Arjomand and Frisvad, Jeppe Revall and Chandraker, Manmohan},
  journal={arXiv preprint arXiv:2501.03717},
  year={2025},
  abstract={Achieving physically consistent image editing remains a significant challenge in computer vision. Existing image editing methods typically rely on neural networks, which struggle to accurately handle shadows and refractions. Conversely, physics-based inverse rendering often requires multi-view optimization, limiting its practicality in single-image scenarios. In this paper, we propose Materialist, a method combining a learning-based approach with physically based progressive differentiable rendering. Given an image, our method leverages neural networks to predict initial material properties. Progressive differentiable rendering is then used to optimize the environment map and refine the material properties with the goal of closely matching the rendered result to the input image. Our approach enables a range of applications, including material editing, object insertion, and relighting, while also introducing an effective method for editing material transparency without requiring full scene geometry. Furthermore, Our envmap estimation method also achieves state-of-the-art performance, further enhancing the accuracy of image editing task. Experiments demonstrate strong performance across synthetic and real-world datasets, excelling even on challenging out-of-domain images.},
  website={https://lez-s.github.io/materialist_project/}
}

@article{wang2025relumix,
  title={ReLumix: Extending Image Relighting to Video via Video Diffusion Models},
  author={Wang, Lezhong and Jin, Shutong and Cui, Ruiqi and Dahl, Anders Bjorholm and Frisvad, Jeppe Revall and Bigdeli, Siavash},
  journal={arXiv preprint arXiv:2509.23769},
  year={2025},
  abstract={Controlling illumination during video post-production is a crucial yet elusive goal in computational photography. Existing methods often lack flexibility, restricting users to certain relighting models. This paper introduces ReLumix, a novel framework that decouples the relighting algorithm from temporal synthesis, thereby enabling any image relighting technique to be seamlessly applied to video. Our approach reformulates video relighting into a simple yet effective two-stage process: (1) an artist relights a single reference frame using any preferred image-based technique (e.g., Diffusion Models, physics-based renderers); and (2) a fine-tuned stable video diffusion (SVD) model seamlessly propagates this target illumination throughout the sequence. To ensure temporal coherence and prevent artifacts, we introduce a gated cross-attention mechanism for smooth feature blending and a temporal bootstrapping strategy that harnesses SVD's powerful motion priors. Although trained on synthetic data, ReLumix shows competitive generalization to real-world videos. The method demonstrates significant improvements in visual fidelity, offering a scalable and versatile solution for dynamic lighting control.},
  website={https://lez-s.github.io/Relumix_project/}
}

@inproceedings{cui2024synthesizing,
  title={Synthesizing 3D Axon Morphology: Springs are All We Need},
  author={Cui, Ruiqi and B{\ae}rentzen, J Andreas and Dyrby, Tim B},
  booktitle={International Workshop on Computational Diffusion MRI},
  pages={35--46},
  year={2024},
  selected={true},
  preview={deformation.png},
  abstract={The realism of digital phantoms for the white matter microstructure is highly valued. Realistic synthesis provides reliable input to generate synthetic diffusion MRI signals for evaluating biophysical models or training machine learning models of microstructure features, such as axon diameter, shapes, and cellular structures. Inspired by the popular spring-mass systems used in physical simulation, we propose a novel and flexible method for synthesizing axon morphology and its dynamics with physical constraints. Specifically, starting with an initial axon configuration, our method constructs a spring-mass system based on specific sampling rules inspired by the real 3D axons and cell morphology observed in X-ray synchrotron imaging. By minimizing the spring potential energy, our method optimizes the positions of sampled mass points, thereby deforming the axon morphology from its physical surroundings. After the optimization, a triangle mesh of the axon surfaces is obtained and can be used as input for Monte Carlo diffusion MRI simulations. Experimental results demonstrate that our approach successfully mimics a range of axon morphologies and the dynamic environment.},
  organization={Springer}
}

@article{cui2024surface,
  title={Surface Reconstruction Using Rotation Systems},
  author={Cui, Ruiqi and G{\ae}de, Emil Toftegaard and Rotenberg, Eva and Kobbelt, Leif and B{\ae}rentzen, J Andreas},
  journal={ACM Transactions on Graphics (TOG)},
  volume={43},
  number={6},
  pages={1--22},
  selected={true},
  abstract={Inspired by the seminal result that a graph and an associated rotation system uniquely determine the topology of a closed manifold, we propose a combinatorial method for reconstruction of surfaces from points. Our method constructs a spanning tree and a rotation system. Since the tree is trivially a planar graph, its rotation system determines a genus zero surface with a single face which we proceed to incrementally refine by inserting edges to split faces and thus merging them. In order to raise the genus, special handles are added by inserting edges between different faces and thus merging them. We apply our method to a wide range of input point clouds in order to investigate its effectiveness, and we compare our method to several other surface reconstruction methods. We find that our method offers better control over outlier classification, i.e. which points to include in the reconstructed surface, and also more control over the topology of the reconstructed surface.},
  website={https://cuirq3.github.io/projects/siga_24/},
  year={2024},
  preview={representative_vn2.png},
  publisher={ACM New York, NY, USA}
}

@article{liu2023pa,
  title={PA-Net: Plane Attention Network for real-time urban scene reconstruction},
  author={Liu, Yilin and Cui, Ruiqi and Xie, Ke and Gong, Minglun and Huang, Hui},
  journal={Computers \& Graphics},
  volume={115},
  pages={254--262},
  year={2023},
  selected={true},
  preview={panet.png},
  publisher={Elsevier}
}

@article{liu2021aerial,
  title={Aerial path planning for online real-time exploration and offline high-quality reconstruction of large-scale urban scenes},
  author={Liu, Yilin and Cui, Ruiqi and Xie, Ke and Gong, Minglun and Huang, Hui},
  journal={ACM Transactions on Graphics (TOG)},
  volume={40},
  number={6},
  pages={1--16},
  year={2021},
  publisher={ACM New York, NY, USA},
  website={https://vcc.tech/research/2021/DroneFly},
  selected={true},
  preview={dronefly.jpg},
  abstract={Existing approaches have shown that, through carefully planning flight trajectories, images captured by Unmanned Aerial Vehicles (UAVs) can be used to reconstruct high-quality 3D models for real environments. These approaches greatly simplify and cut the cost of large-scale urban scene reconstruction. However, to properly capture height discontinuities in urban scenes, all state-of-the-art methods require prior knowledge on scene geometry and hence, additional prepossessing steps are needed before performing the actual image acquisition flights. To address this limitation and to make urban modeling techniques even more accessible, we present a real-time explore-and-reconstruct planning algorithm that does not require any prior knowledge for the scenes. Using only captured 2D images, we estimate 3D bounding boxes for buildings on-the-fly and use them to guide online path planning for both scene exploration and building observation. Experimental results demonstrate that the aerial paths planned by our algorithm in real-time for unknown environments support reconstructing 3D models with comparable qualities and lead to shorter flight air time.}
}

@article{cui2020statistically,
  title={Statistically driven model for efficient analysis of few-photon transport in waveguide quantum electrodynamics},
  author={Cui, Ruiqi and Tan, Dian and Shen, Yuecheng},
  journal={JOSA B},
  volume={37},
  number={2},
  pages={420--424},
  year={2020},
  publisher={Optica Publishing Group},
  abstract={Understanding transport properties in quantum nanophotonics plays a central role in designing few-photon devices, yet it suffers from a longstanding extensive computational burden. In this work, we propose a statistically driven model with a tremendously eased computational burden, which is based on the deep understanding of the few-photon spontaneous emission process. By utilizing phenomenological, statistically driven inter-photon offset parameters, the proposed model expedites the transport calculation with a three-order-of-magnitude enhancement of speed in contrast to conventional numerical approaches. We showcase the two-photon transport computation benchmarked by the rigorous analytical approach. Our work provides an efficient tool for designing few-photon nano-devices, and it significantly deepens the understanding of correlated quantum many-body physics.}
}


